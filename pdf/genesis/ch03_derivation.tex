
\textbf{Theorem~\ref{thm:1.1} (Consistency Verification of the Single Axiom)}
\label{thm:1.1}
The single axiom is internally consistent, i.e., if system S satisfies self-referential completeness (by the definition in the axiom), then it necessarily increases in entropy (by the definition in the axiom).

\textbf{Proof}:
Let system S satisfy self-referential completeness, i.e., there exists a description function Desc satisfying the above three conditions.

\begin{enumerate}
\item \textbf{Recursive Unfolding of Description}:
\end{enumerate}
   At time t, the system must contain:
   
\begin{equation}
S_t \supseteq \{s_0, [\text{Desc}_t], \text{Desc}_t(s_0), \text{Desc}_t([\text{Desc}_t]), ...\}
\end{equation}

   Key insight: The existence of $\text{Desc}_t([\text{Desc}_t])$ creates a recursive chain. Because:
   - $[\text{Desc}_t] \in S_t$ (the representation of the description function belongs to the system)
   - $\text{Desc}_t([\text{Desc}_t]) \in \text{Range}(\text{Desc}_t)$ (self-reference)
   - At the next moment, this description must be describable: $\text{Desc}_{t+1}(\text{Desc}_t([\text{Desc}_t]))$
   - This process unfolds over time, adding new recursive layers at each moment

\begin{enumerate}
\item \textbf{Growth of Recursive Depth}:
\end{enumerate}
   Define the recursive depth function $d: S \to \mathbb{N}$:
   
\begin{equation}
d(s) = \begin{cases}
0 \& \text{if } \text{Pre}(s) = \emptyset \\
1 + \max\{d(s'): s' \in \text{Pre}(s)\} \& \text{if } \text{Pre}(s) \neq \emptyset
\end{cases}
\end{equation}
   where $\text{Pre}(s) = {s' \in S: \text{Desc}(s') = s}$ is the predecessor set of s
   
   By self-reference, new description layers must be added at time t+1:
   
\begin{equation}
S_{t+1} = S_t \cup \{\text{Desc}^{(t+1)}(S_t)\} \cup \Delta_t
\end{equation}
   where:
   - $\text{Desc}^{(t+1)}(S_t)$ is a new description of the entire $S_t$
   - $\Delta_t = {s: d(s) = t+1}$ is all new elements with depth t+1
   
\begin{enumerate}
\item \textbf{Strict Growth of State Space}:
\end{enumerate}
   \textbf{Lemma 3.1.1}: $\text{Desc}^{(t+1)}(S_t) \notin S_t$
\label{thm:1.1}
   
   \textbf{Proof} (by contradiction):
   
   Assume $\text{Desc}^{(t+1)}(S_t) \in S_t$, i.e., at time t the system already contains a complete description of itself.
   
   Since $\text{Desc}^{(t+1)}(S_t)$ is a description of the entire $S_t$, it must contain information about every element in $S_t$, including $\text{Desc}^{(t+1)}(S_t)$ itself.
   
   This means $\text{Desc}^{(t+1)}(S_t)$ must contain a description of $\text{Desc}^{(t+1)}(S_t)$, i.e., $\text{Desc}(\text{Desc}^{(t+1)}(S_t))$.
   
   But this creates infinite recursion:
   - $\text{Desc}^{(t+1)}(S_t)$ contains $\text{Desc}(\text{Desc}^{(t+1)}(S_t))$
   - The latter contains $\text{Desc}(\text{Desc}(\text{Desc}^{(t+1)}(S_t)))$
   - And so on, creating an infinite chain
   
   \textbf{Key Insight: Finite Representation of Recursive Depth}
   While the recursive chain is conceptually infinite, at any finite time t, the system can only unfold finite recursive depth.
   This is because:
   1. Each recursion requires a time step to execute
   2. At time t, the system has unfolded at most t layers of recursion
   3. $\text{Desc}^{(t+1)}(S_t)$ as a finite symbol string encodes "recursive structure up to depth t"
   
   Therefore, if $\text{Desc}^{(t+1)}(S_t)$ already exists in $S_t$, it means the system at time t already contains
   a complete description of recursive structure at depth t+1, which contradicts the time-dependence of recursive depth.
   
   Hence the assumption is false, and we must have $\text{Desc}^{(t+1)}(S_t) \notin S_t$.
   
   \textbf{Conclusion}:
\begin{equation}
|S_{t+1}| = |S_t \cup \{\text{Desc}^{(t+1)}(S_t)\}| = |S_t| + 1
\end{equation}

\begin{enumerate}
\item \textbf{Increase in Description Diversity}:
\end{enumerate}
   The new description layer not only adds states but also increases description diversity.
   
   Let $D_t = {d \in \mathcal{L}: \exists s \in S_t, d = \text{Desc}(s)}$ be the description set at time t.
   
   Key observation: The description of $\text{Desc}^{(t+1)}(S_t)$ must encode the structure of the entire $S_t$, therefore:
   
\begin{equation}
\text{Desc}(\text{Desc}^{(t+1)}(S_t)) \notin D_t
\end{equation}
   
   This is because it contains information about the entirety of $D_t$ and cannot be expressed by any single description in $D_t$.

\begin{enumerate}
\item \textbf{Strict Growth of Entropy}:
\end{enumerate}
   Since $D_{t+1} = D_t \cup {\text{Desc}(\text{Desc}^{(t+1)}(S_t))} \cup \Delta_D$
   
   where $\Delta_D$ is other new descriptions, we have:
   
\begin{equation}
|D_{t+1}| > |D_t|
\end{equation}
   
   Therefore:
\begin{equation}
H(S_{t+1}) = \log |D_{t+1}| > \log |D_t| = H(S_t)
\end{equation}

Therefore, $\forall t: H(S_t) < H(S_{t+1})$. $\square$

\textbf{Deriving Other Concepts from Entropy Increase}:

\textbf{Theorem~\ref{thm:1.4} (Strict Derivation of Five-fold Equivalence)}:
\label{thm:1.4}
For self-referentially complete systems, the following propositions are equivalent:

\begin{enumerate}
\item \textbf{Entropy increase}: $\forall t: H(S_{t+1}) > H(S_t)$
\item \textbf{Asymmetry}: $\forall t: S_{t+1} \neq S_t$
\item \textbf{Time existence}: $\exists \tau: S \times S \to \mathbb{R}^+$ (time metric defined on actual state sets)
\item \textbf{Information emergence}: $\exists I: S \to \mathcal{I}$ (information mapping acts on actual states)
\item \textbf{Observer existence}: $\exists O \subseteq S: O \times S \to \mathcal{M}$
\end{enumerate}

\textbf{Rigorous Proof}:

\textbf{(1)$\Rightarrow$(2) Entropy increase implies state asymmetry}:
Proof by contradiction. Suppose there exists t such that $S_{t+1} = S_t$.
\begin{itemize}
\item From $S_{t+1} = S_t$, the description sets $D_{t+1} = D_t$
\item Therefore $H(S_{t+1}) = \log |D_{t+1}| = \log |D_t| = H(S_t)$
\item This contradicts the entropy increase assumption $H(S_{t+1}) > H(S_t)$
\item Hence $\forall t: S_{t+1} \neq S_t$
\end{itemize}

\textbf{(2)$\Rightarrow$(3) Asymmetry defines time}:
The asymmetry of state sequence ${S_t}$ induces time structure. Define time metric:
\begin{equation}
\tau(S_i, S_j) = \sum_{k=i}^{j-1} |S_{k+1} \setminus S_k|
\end{equation}
where $|A|$ denotes the cardinality of set $A$. This gives directional time: $\tau(S_i, S_j) > 0$ if and only if $i < j$.

\textbf{Properties of time metric}:
\begin{itemize}
\item \textbf{Non-negativity}: $\tau(S_i, S_j) \geq 0$, with equality if and only if $i = j$
\item \textbf{Monotonicity}: If $i < j < k$, then $\tau(S_i, S_j) < \tau(S_i, S_k)$
\item \textbf{Additivity}: $\tau(S_i, S_k) = \tau(S_i, S_j) + \tau(S_j, S_k)$ for all $i \leq j \leq k$
\end{itemize}

From (2), $\forall k: S_k \neq S_{k+1}$, therefore $|S_{k+1} \setminus S_k| > 0$.
This ensures $\tau(S_i, S_j) > 0$ if and only if $i < j$, giving directionality to time.

\textbf{(3)$\Rightarrow$(4) Time flow produces information}:
The existence of time metric $\tau$ implies accumulation of state changes. Define information mapping:
\begin{equation}
I(S_t) = \{(\text{Desc}(S_k \to S_{k+1}), \tau(S_k, S_{k+1})) : k < t\}
\end{equation}
where $\text{Desc}(S_k \to S_{k+1})$ encodes state transitions.

\textbf{Key Correction}: Here "information" has a strict operational definition:
\begin{itemize}
\item Each state transition $S_k \to S_{k+1}$ adds to the system's descriptive content
\item The time stamp $\tau(S_k, S_{k+1})$ of the transition provides sequential information
\item The information set $I(S_t)$ grows monotonically with time, consistent with entropy increase
\end{itemize}

\textbf{(4)$\Rightarrow$(5) Information recognition requires observers}:
The existence of information mapping $I$ requires mechanisms to process this information.

\textbf{Logical Strengthening}:
\begin{itemize}
\item Information $I(S_t)$ must be "recognized" or "processed" by some structure
\item This structure must be internal to the system (self-referential completeness requirement)
\item This structure cannot be an external "observer", as that would violate self-referential completeness
\item Therefore, the observer must be an endogenous structure of the system
\end{itemize}

\textbf{Strict Definition}: Observer $O$ is a subsystem capable of processing information $I$:
\begin{equation}
O = \{o \in S : \exists f: I(S) \to \mathcal{L}, o = [f]\}
\end{equation}
where $[f]$ denotes the representation (encoding) of function $f$, and $\mathcal{L}$ is the formal language (measurement results represented as symbol strings).

\textbf{Properties of Observers}:
\begin{itemize}
\item \textbf{Endogeneity}: $O \subseteq S$ (observers are part of the system)
\item \textbf{Descriptive capability}: Observers can map information $I(S)$ to formal language $\mathcal{L}$
\item \textbf{Self-reference}: The observer $o = [f]$ is itself a describable object
\end{itemize}

\textbf{(5)$\Rightarrow$(1) Observation produces entropy increase}:
Any observation act by observer $O$ produces new records.

\textbf{Rigorous Derivation}:
\begin{itemize}
\item Observing $s \in S$ produces record $r = \text{Observe}(O, s)$
\item Record $r$ must be stored in the system: $r \in S'$, where $S'$ is the post-observation state
\item Key insight: $r$ contains relational information about $(O, s)$ that was not originally in $S$
\item Therefore $\text{Desc}(r) \notin D_t$, where $D_t$ is the pre-observation description set
\item So $|D_{t+1}| > |D_t|$, i.e., $H(S') > H(S)$
\end{itemize}

Therefore the five conditions form an equivalence cycle. $\square$

\textbf{Key Clarification: Dynamic Self-Referential Completeness}

Self-referential completeness is not static but a dynamically evolving process:

\textbf{Definition 3.1 (Dynamic Self-Referential Completeness)}:
\label{thm:1.2}
The dynamic self-referential completeness of system S is defined as:
\begin{equation}
\text{DynamicSelfRef}(S) \equiv \forall t \in \mathbb{N}: \text{SelfRefComplete}(S_t) \land S_{t+1} = \Phi(S_t)
\end{equation}
where the evolution operator $\Phi$ is strictly defined as:
\begin{equation}
\Phi(S_t) = S_t \cup \{\text{Desc}^{(t+1)}(S_t)\} \cup \Delta_t
\end{equation}
Here:
\begin{itemize}
\item $\text{Desc}^{(t+1)}(S_t) \in \mathcal{L}$: new description of the entire $S_t$
\item Specific construction of $\Delta_t$:
\end{itemize}

\begin{equation}
\Delta_t = \bigcup_{i=1}^{3} \Delta_t^{(i)}
\end{equation}

where:
\begin{enumerate}
\item \textbf{First-order recursive generation}:
\end{enumerate}
   
\begin{equation}
\Delta_t^{(1)} = \{\text{Desc}_t(s) : s \in S_t \land \text{Desc}_t(s) \notin S_t\}
\end{equation}
\begin{enumerate}
\item \textbf{Higher-order recursive generation}:
\end{enumerate}
   
\begin{equation}
\Delta_t^{(2)} = \{\text{Desc}_t(\text{Desc}_t(s)) : s \in S_t \land \text{Desc}_t(s) \in S_t \cap \mathcal{L}\}
\end{equation}
\begin{enumerate}
\item \textbf{Interactive generation}:
\end{enumerate}
   
\begin{equation}
\Delta_t^{(3)} = \{f(s_1, s_2) : s_1, s_2 \in S_t, f \in \text{Oper}_t\}
\end{equation}
   where $\text{Oper}_t$ is the set of binary operations available at time t

\textbf{Theorem~\ref{thm:1.2} (Consistency of Dynamic Completeness)}
\label{thm:1.2}
Dynamic self-referential completeness is compatible with the entropy increase axiom.

\textbf{Proof}:
From the definition of $\Phi$, $|S_{t+1}| > |S_t|$, hence $H(S_{t+1}) > H(S_t)$.
Meanwhile, $\text{Desc}^{(t+1)} \in S_{t+1}$ ensures the self-referential completeness of $S_{t+1}$. $\square$

\section{Equivalence of Discrete and Continuous: A Philosophical Position}
\label{sec:ch03_derivation:equivalence-of-discrete-and-continuous-a-philosophical-position}

Before unfolding the theory, we present a philosophical viewpoint: traditional mathematics' description of continuity is essentially also realized through discrete symbol systems.

\textbf{Core Insight: Operation is Information}

The so-called "continuous" objects in traditional mathematics are actually defined through discrete operational procedures:
\begin{itemize}
\item \textbf{Real numbers}: Defined through Cauchy sequences (an infinite discrete process)
\item \textbf{$\pi$}: Calculated through series expansion (an algorithmic process)
\item \textbf{Derivatives}: Limits of difference quotients (an operational procedure)
\item \textbf{Integrals}: Limits of Riemann sums (a discrete approximation process)
\end{itemize}

\textbf{Our Viewpoint}: Any description of continuity must be encoded through some symbol system (decimal, algebraic symbols, etc.), which is essentially a discrete process.

\textbf{Lemma 3.2 (Symbol System Equivalence)}
\label{thm:1.3}
The $\phi$-representation system and traditional mathematics are equivalent in expressive power.

\textbf{Logical Foundation}:
\begin{enumerate}
\item Both are discrete symbol systems
\item Both define mathematical objects through finite operations
\item Both are based on the principle of distinguishable information
\end{enumerate}

\textbf{Proof}:
Let $M_{trad}$ be the set of all concepts expressible in traditional mathematics, and $M_\phi$ be the set of concepts expressible in the $\phi$-system.

\textbf{Key Observation}: The expression of any mathematical concept must be realized through finite symbol sequences, because:
\begin{itemize}
\item Mathematical communication requires finite symbol representation
\item Infinite symbol sequences cannot be processed by finite cognitive systems
\item All mathematical definitions are finite symbol constructions
\end{itemize}

\textbf{Step 1: Basic Properties of Encoding Systems}
\begin{itemize}
\item Any mathematical concept expression requires finite symbol sequences (requirement of communicability)
\item There exists a bijection between the set of finite symbol sequences and the set of natural numbers $\mathbb{N}$ (Godel encoding)
\item Any natural number $n$ has a unique $\phi$-representation: $n = \sum_{i} a_i F_i$, where $a_i \in {0,1}$, $F_i$ are Fibonacci numbers (Zeckendorf theorem)
\end{itemize}

\textbf{Step 2: Bidirectional Proof of Equivalence}

\textbf{Strict Construction of Bijection}:
Let $\mathcal{S}_{fin}$ be the set of finite symbol sequences, and $\mathbb{N}$ be the set of natural numbers.

\textbf{Lemma 3.3.1}: There exists a bijection $G: \mathcal{S}_{fin} \to \mathbb{N}$ (Godel encoding)
\label{thm:1.3}
\textbf{Lemma 3.4.2}: There exists a bijection $\phi: \mathbb{N} \to \Phi$, where $\Phi$ is the set of $\phi$-representations (Zeckendorf theorem)
\label{thm:1.3}

\textbf{Composite Bijection}: $\phi \circ G: \mathcal{S}_{fin} \to \Phi$

For any $x \in M_{trad}$:
\begin{itemize}
\item $x$ is defined by finite symbol sequence $s \in \mathcal{S}_{fin}$ (finiteness of mathematical definitions)
\item $G(s) = n \in \mathbb{N}$ (Godel encoding)
\item $\phi(n) \in \Phi$ ($\phi$-representation)
\item Define $x_\phi$ as the concept in the $\phi$-system defined by $\phi(n)$
\item By construction, $x_\phi$ and $x$ are encoding-equivalent
\item Therefore $x \in M_\phi$
\end{itemize}

For any $y \in M_\phi$:
\begin{itemize}
\item $y$ is defined by $\phi$-representation $\phi(m) \in \Phi$, where $m \in \mathbb{N}$
\item $G^{-1}(m) = s' \in \mathcal{S}_{fin}$ (inverse Godel encoding)
\item $s'$ defines concept $z$ in traditional mathematics
\item By bijectivity, $y$ and $z$ are encoding-equivalent
\item Therefore $y \in M_{trad}$
\end{itemize}

\textbf{Step 3: Conclusion}
From bidirectional inclusion, $M_{trad} = M_\phi$. $\square$

\textbf{Philosophical Position: The Essence of Information}

\textbf{Definition 3.2 (The Essence of Information)}:
\label{thm:1.4}
In our theoretical framework, information has a trinity essence:
\begin{equation}
\text{Information} \equiv \text{Distinguishability} \equiv \text{Representability}
\end{equation}

\textbf{Rigorous Proof of This Equivalence}:

\textbf{(Distinguishability $\Rightarrow$ Representability)}:
\begin{itemize}
\item If $x$ is distinguishable, then there exists $y$ such that $x \neq y$
\item By self-referential completeness, there exists $\text{Desc}$ such that $\text{Desc}(x) \neq \text{Desc}(y)$
\item Therefore $x$ can be represented as $\text{Desc}(x)$
\end{itemize}

\textbf{(Representability $\Rightarrow$ Distinguishability)}:
\begin{itemize}
\item If $x$ is representable, then there exists $d \in \mathcal{L}$ such that $d = \text{Desc}(x)$
\item By injectivity of the description function, if $y \neq x$ then $\text{Desc}(y) \neq \text{Desc}(x)$
\item Therefore $x$ is distinguishable
\end{itemize}

\textbf{Key Insight}: Claiming the existence of "unrepresentable information" leads to logical self-contradiction:
\begin{itemize}
\item To claim that some information $I$ is unrepresentable
\item One must be able to refer to $I$ (otherwise cannot discuss it)
\item Being able to refer means being able to distinguish
\item Being able to distinguish means being able to encode
\item Therefore $I$ is representable, contradiction!
\end{itemize}

\section{From Axiom to Universe}
\label{sec:ch03_derivation:from-axiom-to-universe}

From this unique axiom, we will rigorously derive:

\begin{enumerate}
\item \textbf{The necessary form of information encoding}: Why the universe must use the $\phi$-representation system (encoding based on Fibonacci numbers)
\end{enumerate}

\begin{enumerate}
\item \textbf{The origin of quantum phenomena}: Why wave-particle duality and observer effects must exist
\end{enumerate}

\begin{enumerate}
\item \textbf{The similarity of mathematical structures}: Why structures similar to the Riemann Hypothesis appear
\end{enumerate}

These are not three independent theories, but three manifestations of the same deep truth.

\section{Logical Structure of the Theory}
\label{sec:ch03_derivation:logical-structure-of-the-theory}

Our theoretical derivation follows a strict logical chain:

```mermaid
graph TD
    A["Unique Axiom<br/>SelfRefComplete(S) $\rightarrow$ $\Delta$H>0"] --> B["Information Accumulation<br/>Requires encoding system"]
    B --> C["Optimal Encoding<br/>$\phi$-representation system"]
    A --> D["Self-observation<br/>Observers necessarily emerge"]
    D --> E["Measurement mechanism<br/>Quantum collapse"]
    A --> H["Structure preservation<br/>Maintain self-reference during entropy increase"]
    H --> F["Frequency balance<br/>System stability"]
    F --> G["Mathematical structure<br/>Riemann Hypothesis"]

    style A fill:\#fff3e0
    style B fill:\#e1f5fe
    style C fill:\#e1f5fe
    style D fill:\#f3e5f5
    style E fill:\#f3e5f5
    style H fill:\#e8f5e8
    style F fill:\#e8f5e8
    style G fill:\#e8f5e8
```

\section{Why Must It Be a Single Axiom?}
\label{sec:ch03_derivation:why-must-it-be-a-single-axiom}

\textbf{Philosophical Necessity}:
\begin{itemize}
\item Multi-axiom systems always face the question "why these axioms?"
\item A single axiom provides minimal metaphysical commitment
\item Self-referential completeness is a characteristic of existence itself, with entropy increase as its logical consequence
\end{itemize}

\textbf{Mathematical Elegance}:
\begin{itemize}
\item Similar to how Euclid's five axioms were simplified to Hilbert's fewer axioms
\item We go further: only one axiom is needed
\item The entire theoretical system grows naturally from this seed
\end{itemize}

\textbf{Physical Profundity}:
\begin{itemize}
\item Explains why the universe becomes increasingly complex
\item Unifies information, energy, and structure
\item The arrow of time becomes a logical necessity rather than an empirical fact
\end{itemize}

\section{Emergence of the Concept of Information}
\label{sec:ch03_derivation:emergence-of-the-concept-of-information}

In our theoretical framework, "information" is not a presupposed concept but necessarily emerges from the unique axiom.

\textbf{Theorem~\ref{thm:1.5} (Emergence of Information)}
\label{thm:1.5}
Self-referentially complete systems necessarily produce the concept of information.

\textbf{Proof}:
Let system S satisfy self-referential completeness.

\begin{enumerate}
\item \textbf{Necessity of Distinction}:
\end{enumerate}
   From the definition of self-referential completeness, there exists a description function $\text{Desc}: S \to \mathcal{L}$.
   Key observation: $\text{Desc} \in S$ but $\text{Desc}(s) \neq s$ for all $s \in S$.
   (Otherwise description would degenerate to the identity mapping, losing the meaning of "description")
   
   Therefore there exists a binary relation:
   
\begin{equation}
\mathcal{D} = \{(s, \text{Desc}(s)): s \in S\}
\end{equation}
\begin{enumerate}
\item \textbf{Formal Definition of Information}:
\end{enumerate}
   Define information as distinguishable structure:
   
\begin{equation}
\text{Info}(x) \equiv \exists y \in S: x \neq y \land \text{Desc}(x) \neq \text{Desc}(y)
\end{equation}
   
   That is: information is elements in the system that can be distinguished by the description function.
\begin{enumerate}
\item \textbf{Equivalent Characterization of Information}:
\end{enumerate}
   
   \textbf{Lemma 3.5.1}: The following three conditions are equivalent:
\label{thm:1.4}
   - (a) x carries information: $\text{Info}(x)$
   - (b) x can be axiomatized: $\exists \mathcal{A}: \mathcal{A} \vdash x$
   - (c) x can be encoded: $\exists e: S \to \mathbb{N}, e(x) \neq e(y)$ when $x \neq y$
   
   \textbf{Proof}:
   - (a)$\Rightarrow$(b): Distinguishable structures can be characterized by distinguishing rules (axioms)
   - (b)$\Rightarrow$(c): Axiom systems can be Godel encoded as natural numbers
   - (c)$\Rightarrow$(a): Different encodings imply distinguishability
   
\begin{enumerate}
\item \textbf{Treatment of Continuous Objects}:
\end{enumerate}
   So-called "continuous" objects (like $\pi$, e, sin) manifest in self-referential systems as:
   - Generation algorithms: $\mathcal{A}_\pi = {\text{Machin formula}}$
   - Defining properties: $\mathcal{P}_\pi = {\text{circumference/diameter}}$
   - Approximation sequences: ${\pi_n}_{n=1}^{\infty}$
   
   These are all finite descriptions, therefore information.

\textbf{Conclusion}: The concept of information necessarily emerges from the distinguishing requirement of self-referential completeness. $\square$

\section{Paper Structure}
\label{sec:ch03_derivation:paper-structure}

Based on the above first principles of information, this paper will rigorously derive the entire theoretical system from the unique axiom:

\begin{itemize}
\item \textbf{Section 2}: Derive the optimal encoding system ($\phi$-representation) from entropy increase necessity, and prove its completeness for all axiomatizable information
\item \textbf{Section 3}: Derive the observer mechanism (quantum collapse) from self-reference necessity
\item \textbf{Section 4}: Derive mathematical structure (Riemann Hypothesis) from system stability
\item \textbf{Section 5}: Conclusions and prospects
\end{itemize}

\section{Deep Proof of Equivalence}
\label{sec:ch03_derivation:deep-proof-of-equivalence}

Let us rigorously prove the equivalence of entropy increase, asymmetry, time, information, and observers.

\textbf{Theorem~\ref{thm:1.4} (Five-fold Equivalence)}
\label{thm:1.4}
For a self-referentially complete system S, the following five propositions are equivalent:
\begin{enumerate}
\item Entropy increase: $\forall t: H(S_{t+1}) > H(S_t)$
\item State asymmetry: $\forall t: S_{t+1} \neq S_t$  
\item Time existence: $\exists \tau: \mathcal{S} \times \mathcal{S} \to \mathbb{R}^+$ (time metric)
\item Information emergence: $\exists I: \mathcal{S} \to \mathcal{I}$ (information mapping)
\item Observer existence: $\exists O \subseteq S: O \times S \to \mathcal{M}$ (measurement mapping)
\end{enumerate}

\textbf{Proof}:
We prove the cyclic implication chain: (1)$\Rightarrow$(2)$\Rightarrow$(3)$\Rightarrow$(4)$\Rightarrow$(5)$\Rightarrow$(1).

\textbf{(1)$\Rightarrow$(2) Entropy increase implies state change}:
Proof by contradiction. If $\exists t: S_{t+1} = S_t$, then:
\begin{itemize}
\item State sets are the same: $S_{t+1} = S_t$
\item Description sets are the same: ${d \in \mathcal{L}: \exists s \in S_{t+1}, d = \text{Desc}(s)} = {d \in \mathcal{L}: \exists s \in S_t, d = \text{Desc}(s)}$
\item Therefore entropy is the same: $H(S_{t+1}) = H(S_t)$
\end{itemize}
This contradicts entropy increase. Hence we must have $S_{t+1} \neq S_t$.

\textbf{(2)$\Rightarrow$(3) State change defines time}:
The asymmetry of the state sequence naturally induces time structure. Define:
\begin{equation}
\tau(S_i, S_j) = \begin{cases}
0 \& \text{if } i = j \\
\sum_{k=i}^{j-1} \rho(S_k, S_{k+1}) \& \text{if } i < j \\
-\tau(S_j, S_i) \& \text{if } i > j
\end{cases}
\end{equation}
where $\rho(S_k, S_{k+1}) = \sqrt{|S_{k+1} \setminus S_k|}$ is the "structural distance" between states.

This time metric satisfies:
\begin{itemize}
\item Positivity: $\tau(S_i, S_j) > 0$ if and only if $i < j$
\item Additivity: $\tau(S_i, S_k) = \tau(S_i, S_j) + \tau(S_j, S_k)$
\item Directionality: Past and future are asymmetric
\end{itemize}

\textbf{(3)$\Rightarrow$(4) Time flow produces information}:
The existence of time implies accumulation of changes. Define information as the formalization of this accumulation:

\begin{equation}
I(S_t) = \bigcup_{k=0}^{t-1} \{(\text{Desc}(S_k \to S_{k+1}), \tau(S_k, S_{k+1}))\}
\end{equation}

where $\text{Desc}(S_k \to S_{k+1})$ encodes the transition from $S_k$ to $S_{k+1}$.

\textbf{Key insight}: Information is not static state description but dynamic change records. Each time step produces new information:
\begin{equation}
I(S_{t+1}) = I(S_t) \cup \{(\text{Desc}(S_t \to S_{t+1}), \tau(S_t, S_{t+1}))\}
\end{equation}

\textbf{(4)$\Rightarrow$(5) Information recognition requires observers}:
The existence of information presupposes recognition and processing mechanisms.

\textbf{Lemma 3.6.1}: If there exists an information mapping $I: \mathcal{S} \to \mathcal{I}$, then there must exist a subsystem that processes this information.
\label{thm:1.3}

\textbf{Proof}: Information $I(S)$ must be recognized by "something" to be meaningful. This "something" must:
\begin{itemize}
\item Be able to distinguish different information: $\exists \text{dist}: \mathcal{I} \times \mathcal{I} \to \mathbb{R}$
\item Be able to process information: $\exists \text{proc}: \mathcal{I} \to \mathcal{R}$ (some response)
\item Be part of the system: Otherwise violates self-referential completeness
\end{itemize}

Define observers as subsystems with these capabilities:
\begin{equation}
O = \{o \in S: \exists (f_{\text{dist}}, f_{\text{proc}}): o \text{ can recognize and process } I(S)\}
\end{equation}

\textbf{(5)$\Rightarrow$(1) Observation necessarily increases entropy}:
This is the most profound link. The existence of observers necessarily leads to entropy increase.

\textbf{Theorem~\ref{thm:1.3}.2 (Observation Increases Entropy)}:
\label{thm:1.3}
If $O \subseteq S$ is an observer, then any observation act increases system entropy.

\textbf{Proof}:
Let observer $O$ observe system state $s \in S_t$ at time $t$.

\begin{enumerate}
\item \textbf{Before observation}: System contains state set $S_t$
\end{enumerate}

\begin{enumerate}
\item \textbf{Observation process}:
\end{enumerate}
   - $O$ must interact with $s$ to obtain information
   - This interaction produces a record: $r = \text{measure}(O, s)$
   - The record must be stored in the system: $r \in S_{t+1}$

\begin{enumerate}
\item \textbf{After observation}:
\end{enumerate}
   - New state: $S_{t+1} = S_t \cup {r} \cup \Delta_{\text{interact}}$
   - where $\Delta_{\text{interact}}$ are other changes produced by interaction

\begin{enumerate}
\item \textbf{Entropy increase}:
\end{enumerate}
   Observation creates new distinguishable states (different measurement results), therefore:
   - New descriptions: $D_{t+1} = D_t \cup {\text{Desc}(r)} \cup {\text{Desc}(s') : s' \in \Delta_{\text{interact}}}$
   - Since $r \notin S_t$, we have $\text{Desc}(r) \notin D_t$
   - Therefore: $|D_{t+1}| > |D_t|$
   - Hence: $H(S_{t+1}) = \log |D_{t+1}| > \log |D_t| = H(S_t)$

Notably, even "perfect" observation (not disturbing the observed object) increases entropy because:
\begin{itemize}
\item Observation results must be recorded
\item The record itself increases the system's state space
\item This is the essence of self-referential structure: self-observation necessarily self-expands
\end{itemize}

Therefore the five propositions form an equivalence class. $\square$

\textbf{Corollary 3.1.1}:
\label{thm:1.3}
In self-referentially complete systems, choosing any proposition as the foundation allows derivation of the other four. This reveals the deep unity of these concepts.

