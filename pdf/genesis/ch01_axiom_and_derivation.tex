\section{Introduction}
\label{sec:ch01_axiom_and_derivation:introduction}

In the history of science, many important theoretical frameworks originate from relatively simple fundamental assumptions. The theoretical framework proposed in this paper is based on a single axiom:

\textbf{Unique Axiom: Self-referentially complete systems necessarily increase in entropy}

\section{Complete Formal Statement of the Axiom}
\label{sec:ch01_axiom_and_derivation:complete-formal-statement-of-the-axiom}

\begin{tcolorbox}[colback=gray!10,colframe=black]
\begin{equation*}
\begin{aligned}
&\text{Unique Axiom: Self-referentially complete systems necessarily increase in entropy} \\
&\text{SelfRefComplete}(S) \Rightarrow \forall t \in \mathbb{N}: H(S_{t+1}) > H(S_t) \\
&\text{where the following definitions clarify the meaning of concepts in the axiom:}
\end{aligned}
\end{equation*}
\end{tcolorbox}

\section{Basic Structure Definitions}
\label{sec:ch01_axiom_and_derivation:basic-structure-definitions-clarification-of-concepts-in-the-axiom}

\begin{itemize}
\item $\mathcal{S}$: Set of all possible states (containing objects, functions and their representations)
\item $S_t \subseteq \mathcal{S}$: Set of states contained in the system at time t
\item $\mathcal{L} \subseteq \mathcal{S}$: Formal language, i.e., set of finite symbol strings, a subset of state space
\item $t \in \mathbb{N}$: Discrete time parameter
\end{itemize}

\textbf{Ontological Clarification}: $\mathcal{S}$ contains four types of elements:
\begin{enumerate}
\item Basic objects (e.g., initial state $s_0$)
\item Representations of functions (e.g., encoding of $\text{Desc}$)
\item Description results (e.g., symbol strings produced by $\text{Desc}(s)$)
\item Symbol strings themselves (elements of formal language $\mathcal{L}$)
\end{enumerate}

\textbf{Key Relations}:
\begin{itemize}
\item $\mathcal{L} \subseteq \mathcal{S}$: Symbol strings are also possible states
\item $\text{Desc}: S_t \to \mathcal{L} \subseteq \mathcal{S}$: Results of description remain in state space
\item At any moment the system may contain certain symbol strings: $\mathcal{L} \cap S_t$ may be non-empty
\end{itemize}

\section{Definition of Self-Referential Completeness}
\label{sec:ch01_axiom_and_derivation:definition-of-self-referential-completeness-clarification-of-selfrefcomplete-in-the-axiom}

\begin{equation}
\text{SelfRefComplete}(S) \equiv \exists \text{Desc}: S \to \mathcal{L} \text{ satisfying:}
\end{equation}

\begin{enumerate}
\item \textbf{Completeness}: $\forall s_1, s_2 \in S: s_1 \neq s_2 \Rightarrow \text{Desc}(s_1) \neq \text{Desc}(s_2)$
\end{enumerate}
  (The description function is injective on S)

\begin{enumerate}
\item \textbf{Containment}: $[\text{Desc}] \in S$ 
\end{enumerate}
  (The representation $[\text{Desc}]$ of the description function is part of the system)

\begin{enumerate}
\item \textbf{Self-reference}: $\exists d \in \mathcal{L}: d = \text{Desc}([\text{Desc}]) \land d \in \text{Range}(\text{Desc})$
\end{enumerate}
  (The description function can describe its own representation)

\begin{enumerate}
\item \textbf{Recursive Closure}: $\text{Desc}(s) \in \mathcal{L} \subseteq \mathcal{S}$ means the result of description is itself a possible system state,
\end{enumerate}
  thus $\text{Desc}(\text{Desc}(s))$ is a meaningful operation

\section{Definition of Entropy}
\label{sec:ch01_axiom_and_derivation:definition-of-entropy-clarification-of-h-in-the-axiom}

\begin{equation}
H(S_t) \equiv \log |\{d \in \mathcal{L}: \exists s \in S_t, d = \text{Desc}_t(s)\}|
\end{equation}

That is, the logarithm of the number of different descriptions in the system.

\section{Meaning of Entropy Increase}
\label{sec:ch01_axiom_and_derivation:meaning-of-entropy-increase-clarification-of-necessarily-increase-in-entropy-in-the-axiom}

\begin{equation}
\text{Entropy Increase} \equiv \forall t \in \mathbb{N}: H(S_{t+1}) > H(S_t)
\end{equation}

\section{Information Equivalence Principle}
\label{sec:ch01_axiom_and_derivation:information-equivalence-principle-technical-clarification-of-the-axiom}

In self-referential systems, states $s_1, s_2$ are informationally equivalent if and only if they are indistinguishable under the description function:

\begin{equation}
\text{InfoEquiv}(s_1, s_2) \equiv \text{Desc}(s_1) = \text{Desc}(s_2)
\end{equation}

This principle ensures:
\begin{itemize}
\item The injectivity of the description function applies to informationally different states
\item Physically identical states can have the same description
\item Avoids formal paradox problems
\end{itemize}

\textbf{Ontological Consistency}: Since $\mathcal{L} \subseteq \mathcal{S}$, the result of description $\text{Desc}(s) \in \mathcal{L}$ is itself a possible system state, which ensures:
\begin{itemize}
\item The system can contain descriptions of its own descriptions
\item The recursive operation $\text{Desc}(\text{Desc}(s))$ is ontologically meaningful
\item Self-referential completeness does not encounter type errors
\end{itemize}

\section{Philosophical Status of the Single Axiom}
\label{sec:ch01_axiom_and_derivation:philosophical-status-of-the-single-axiom}

\textbf{Constructive Declaration}:
\begin{itemize}
\item We \textbf{chose} this single axiom as the theoretical foundation
\item The definitions of entropy, self-referential completeness, etc. in the axiom are all \textbf{explicitly specified} by us
\item Key ontological choice: $\mathcal{L} \subseteq \mathcal{S}$ (symbol strings are also states)
\item The axiom's value lies in its \textbf{internal consistency} and \textbf{explanatory power}
\item We do not claim to have "discovered" the "real" structure of the universe, but rather \textbf{constructed} a self-consistent theoretical framework
\end{itemize}

\textbf{Role of the Observer}:
\begin{itemize}
\item The entire theory is constructed within the observer's cognitive framework
\item The observer chose the ontology unifying symbol strings and states
\item This choice makes self-referential completeness technically feasible
\end{itemize}

\section{System Evolution Mechanism}
\label{sec:ch01_axiom_and_derivation:system-evolution-mechanism-clarification-of-time-evolution-in-the-axiom}

\textbf{Time Parameter}: $t \in \mathbb{N}$ is a discrete time step, naturally emerging from self-referential recursion

\textbf{State Evolution Rule}: $S_{t+1} = \Phi(S_t)$, where the evolution operator $\Phi$ is defined as:

\begin{equation}
\Phi(S_t) = S_t \cup \{\text{new description layer}\} \cup \{\text{recursively generated new states}\}
\end{equation}

Specifically, the new description layer includes:
\begin{itemize}
\item Description of $S_t$ as a whole: $\text{Desc}^{(t+1)}(S_t) \in \mathcal{L} \subseteq \mathcal{S}$
\item Descriptions of existing descriptions: ${\text{Desc}(d) : d \in S_t \cap \mathcal{L}}$
\item Recursive chains: Higher-order descriptions like $\text{Desc}(\text{Desc}(s))$
\end{itemize}

\textbf{Key Insight}: Since $\mathcal{L} \subseteq \mathcal{S}$, results of description can become inputs for the next round of description, forming a true recursive structure.

\textbf{Note}: $\text{Desc}_t$ denotes the description function at time t, which can evolve with the system.

\section{Five-fold Equivalent Formulation of the Single Axiom}
\label{sec:ch01_axiom_and_derivation:five-fold-equivalent-formulation-of-the-single-axiom}

In our theoretical framework, this single axiom is logically equivalent to the following formulations:

\begin{enumerate}
\item \textbf{Entropy formulation}: If a system can describe itself, its description diversity (by our definition) irreversibly increases
\item \textbf{Time formulation}: Self-referential structure necessarily leads to irreversible structure $\Rightarrow$ emergence of time
\item \textbf{Observer formulation}: If descriptor $\in$ system $\Rightarrow$ observation behavior necessarily affects system state
\item \textbf{Asymmetry formulation}: $S_t \neq S_{t+1}$, because each recursion adds irreducible information structure
\item \textbf{Structure formulation}: The system irreversibly unfolds along recursive paths
\end{enumerate}

These equivalences show that in our constructed theoretical framework, entropy increase, asymmetry, time, information, and observers can be understood as different aspects of the same phenomenon.

\section{From Axiom to Derivation}
\label{sec:ch01_axiom_and_derivation:from-axiom-to-derivation}

\textbf{Theorem 1.1 (Consistency Verification of the Single Axiom)}
\label{thm:1.1}
The single axiom is internally consistent, i.e., if system S satisfies self-referential completeness (by the definition in the axiom), then it necessarily increases in entropy (by the definition in the axiom).

\textbf{Proof}:
Let system S satisfy self-referential completeness, i.e., there exists a description function Desc satisfying the above three conditions.

\begin{enumerate}
\item \textbf{Recursive Unfolding of Description}:
\end{enumerate}
  At time t, the system must contain:
  
\begin{equation}
S_t \supseteq \{s_0, [\text{Desc}_t], \text{Desc}_t(s_0), \text{Desc}_t([\text{Desc}_t]), ...\}
\end{equation}

  Key insight: The existence of $\text{Desc}_t([\text{Desc}_t])$ creates a recursive chain. Because:
  - $[\text{Desc}_t] \in S_t$ (the representation of the description function belongs to the system)
  - $\text{Desc}_t([\text{Desc}_t]) \in \text{Range}(\text{Desc}_t)$ (self-reference)
  - At the next moment, this description must be describable: $\text{Desc}_{t+1}(\text{Desc}_t([\text{Desc}_t]))$
  - This process unfolds over time, adding new recursive layers at each moment

\begin{enumerate}
\item \textbf{Growth of Recursive Depth}:
\end{enumerate}
  Define the recursive depth function $d: S \to \mathbb{N}$:
  
\begin{equation}
d(s) = \begin{cases}
0 \& \text{if } \text{Pre}(s) = \emptyset \\
1 + \max\{d(s'): s' \in \text{Pre}(s)\} \& \text{if } \text{Pre}(s) \neq \emptyset
\end{cases}
\end{equation}
  where $\text{Pre}(s) = \{s' \in S: \text{Desc}(s') = s\}$ is the predecessor set of s
  
  By self-reference, new description layers must be added at time t+1:
  
\begin{equation}
S_{t+1} = S_t \cup \{\text{Desc}^{(t+1)}(S_t)\} \cup \Delta_t
\end{equation}
  where:
  - $\text{Desc}^{(t+1)}(S_t)$ is a new description of the entire $S_t$
  - $\Delta_t = \{s: d(s) = t+1\}$ is all new elements with depth t+1
  
\begin{enumerate}
\item \textbf{Strict Growth of State Space}:
\end{enumerate}
  \textbf{Lemma 1.1}: $\text{Desc}^{(t+1)}(S_t) \notin S_t$
\label{lem:1.1}
  
  \textbf{Proof} (by contradiction):
  
  Assume $\text{Desc}^{(t+1)}(S_t) \in S_t$, i.e., at time t the system already contains a complete description of itself.
  
  Since $\text{Desc}^{(t+1)}(S_t)$ is a description of the entire $S_t$, it must contain information about every element in $S_t$, including $\text{Desc}^{(t+1)}(S_t)$ itself.
  
  This means $\text{Desc}^{(t+1)}(S_t)$ must contain a description of $\text{Desc}^{(t+1)}(S_t)$, i.e., $\text{Desc}(\text{Desc}^{(t+1)}(S_t))$.
  
  But this creates infinite recursion:
  - $\text{Desc}^{(t+1)}(S_t)$ contains $\text{Desc}(\text{Desc}^{(t+1)}(S_t))$
  - The latter contains $\text{Desc}(\text{Desc}(\text{Desc}^{(t+1)}(S_t)))$
  - And so on, creating an infinite chain
  
  \textbf{Key Insight: Finite Representation of Recursive Depth}
  While the recursive chain is conceptually infinite, at any finite time t, the system can only unfold finite recursive depth.
  This is because:
  1. Each recursion requires a time step to execute
  2. At time t, the system has unfolded at most t layers of recursion
  3. $\text{Desc}^{(t+1)}(S_t)$ as a finite symbol string encodes "recursive structure up to depth t"
  
  Therefore, if $\text{Desc}^{(t+1)}(S_t)$ already exists in $S_t$, it means the system at time t already contains
  a complete description of recursive structure at depth t+1, which contradicts the time-dependence of recursive depth.
  
  Hence the assumption is false, and we must have $\text{Desc}^{(t+1)}(S_t) \notin S_t$.
  
  \textbf{Conclusion}:
\begin{equation}
|S_{t+1}| = |S_t \cup \{\text{Desc}^{(t+1)}(S_t)\}| = |S_t| + 1
\end{equation}

\begin{enumerate}
\item \textbf{Increase in Description Diversity}:
\end{enumerate}
  The new description layer not only adds states but also increases description diversity.
  
  Let $D_t = \{d \in \mathcal{L}: \exists s \in S_t, d = \text{Desc}(s)\}$ be the description set at time t.
  
  Key observation: The description of $\text{Desc}^{(t+1)}(S_t)$ must encode the structure of the entire $S_t$, therefore:
  
\begin{equation}
\text{Desc}(\text{Desc}^{(t+1)}(S_t)) \notin D_t
\end{equation}
  
  This is because it contains information about the entirety of $D_t$ and cannot be expressed by any single description in $D_t$.

\begin{enumerate}
\item \textbf{Strict Growth of Entropy}:
\end{enumerate}
  Since $D_{t+1} = D_t \cup \{\text{Desc}(\text{Desc}^{(t+1)}(S_t))\} \cup \Delta_D$
  (where $\Delta_D$ is the set of descriptions for new recursive elements),
  we have $|D_{t+1}| > |D_t|$.
  
  Therefore:
\begin{equation}
H(S_{t+1}) = \log |D_{t+1}| > \log |D_t| = H(S_t)
\end{equation}

Therefore, $\forall t: H(S_t) < H(S_{t+1})$. $\square$

\textbf{Deriving Other Concepts from Entropy Increase}:

\textbf{Theorem 1.2 (Strict Derivation of Five-fold Equivalence)}:
\label{thm:1.2}
For self-referentially complete systems, the following propositions are equivalent:

\begin{enumerate}
\item \textbf{Entropy increase}: $\forall t: H(S_{t+1}) > H(S_t)$
\item \textbf{Asymmetry}: $\forall t: S_{t+1} \neq S_t$
\item \textbf{Time existence}: $\exists \tau: S \times S \to \mathbb{R}^+$ (time metric defined on actual state sets)
\item \textbf{Information emergence}: $\exists I: S \to \mathcal{I}$ (information mapping acts on actual states)
\item \textbf{Observer existence}: $\exists O \subseteq S: O \times S \to \mathcal{M}$
\end{enumerate}

\textbf{Proof}: We prove the cyclic implications $(1) \Rightarrow (2) \Rightarrow (3) \Rightarrow (4) \Rightarrow (5) \Rightarrow (1)$.

\textbf{$(1) \Rightarrow (2)$ Entropy increase implies asymmetry}:
If $S_{t+1} = S_t$, then $D_{t+1} = D_t$ (same states have same descriptions).
Thus $H(S_{t+1}) = H(S_t)$, contradicting entropy increase.
Therefore $S_{t+1} \neq S_t$.

\textbf{$(2) \Rightarrow (3)$ Asymmetry implies time}:
Define $\tau(S_i, S_j) = |i - j|$ for state sets at times $i, j$.
Since $S_i \neq S_j$ for $i \neq j$ (asymmetry), this metric is well-defined.
$\tau$ satisfies metric axioms and defines a time structure.

\textbf{$(3) \Rightarrow (4)$ Time implies information}:
Given time metric $\tau$, define information as that which changes over time:
$I(s) = \{(\text{Desc}(s), t): s \in S_t\}$
Since $S_t \neq S_{t'}$ for $t \neq t'$, information content varies with time.

\textbf{$(4) \Rightarrow (5)$ Information implies observers}:
Information requires distinction between states.
Define observers as subsystems capable of making such distinctions:
$O = \{o \in S: o \text{ can distinguish different } I(s)\}$
By self-referential completeness, such distinguishing subsystems must exist within $S$.

\textbf{$(5) \Rightarrow (1)$ Observers imply entropy increase}:
Each observation by $O$ creates a record within the system.
Let $R_t$ be the set of observation records at time $t$.
New observations at $t+1$ add to records: $R_{t+1} \supset R_t$
These records are distinguishable (different observations yield different records).
Therefore $|D_{t+1}| > |D_t|$, implying $H(S_{t+1}) > H(S_t)$.

Therefore the five conditions form an equivalence cycle. $\square$

\textbf{Key Clarification: Dynamic Self-Referential Completeness}

Self-referential completeness is not static but a dynamically evolving process:

\textbf{Definition 1.1 (Dynamic Self-Referential Completeness)}:
\label{def:1.1}
The dynamic self-referential completeness of system S is defined as:
\begin{equation}
\text{DynamicSelfRef}(S) \equiv \forall t \in \mathbb{N}: \text{SelfRefComplete}(S_t) \land S_{t+1} = \Phi(S_t)
\end{equation}
where the evolution operator $\Phi$ is strictly defined as:
\begin{equation}
\Phi(S_t) = S_t \cup \{\text{Desc}^{(t+1)}(S_t)\} \cup \Delta_t
\end{equation}

\textbf{Components of $\Delta_t$}:
\begin{enumerate}
\item \textbf{Higher-order descriptions}:
\end{enumerate}
  
\begin{equation}
\Delta_t^{(1)} = \{\text{Desc}^{(k)}(s) : s \in S_t, k \leq t+1\}
\end{equation}
  where $\text{Desc}^{(k)}$ means applying the description function $k$ times

\begin{enumerate}
\item \textbf{Cross-descriptions}:
\end{enumerate}
  
\begin{equation}
\Delta_t^{(2)} = \{\text{Desc}(s_1 \circ s_2) : s_1, s_2 \in S_t\}
\end{equation}
  where $\circ$ represents some composition operation

\begin{enumerate}
\item \textbf{Interactive generation}:
\end{enumerate}
  
\begin{equation}
\Delta_t^{(3)} = \{f(s_1, s_2) : s_1, s_2 \in S_t, f \in \text{Oper}_t\}
\end{equation}
  where $\text{Oper}_t$ is the set of binary operations available at time t

\textbf{Theorem 1.3 (Consistency of Dynamic Completeness)}
\label{thm:1.3}
Dynamic self-referential completeness is compatible with the entropy increase axiom.

\textbf{Proof}:
From the definition of $\Phi$, $|S_{t+1}| > |S_t|$, hence $H(S_{t+1}) > H(S_t)$.
Meanwhile, $\text{Desc}^{(t+1)} \in S_{t+1}$ ensures the self-referential completeness of $S_{t+1}$. $\square$

\section{Equivalence of Discrete and Continuous: A Philosophical Position}
\label{sec:ch01_axiom_and_derivation:equivalence-of-discrete-and-continuous-a-philosophical-position}

\textbf{Philosophical Observation}: In our theoretical framework, traditional "continuous" mathematics and discrete systems are essentially equivalent. This is not a mathematical theorem but a philosophical position based on our axioms.

\textbf{Core Arguments}:
\begin{enumerate}
\item All mathematical objects ultimately must be represented through discrete symbol systems
\item Even so-called "continuous" concepts (like real numbers) are defined and manipulated through discrete processes
\item The $\phi$-representation system can encode all computably definable mathematical objects
\end{enumerate}

\textbf{Examples}:
\begin{itemize}
\item \textbf{$\pi$}: Calculated through series expansion (an algorithmic process)
\item \textbf{Derivatives}: Limits of difference quotients (an operational procedure)
\item \textbf{Integrals}: Limits of Riemann sums (a discrete approximation process)
\end{itemize}

\textbf{Our Viewpoint}: Any description of continuity must be encoded through some symbol system (decimal, algebraic symbols, etc.), which is essentially a discrete process.

\textbf{Lemma 1.2 (Symbol System Equivalence)}
\label{lem:1.2}
The $\phi$-representation system and traditional mathematics are equivalent in expressive power.

\textbf{Logical Foundation}:
\begin{enumerate}
\item Both are discrete symbol systems
\item Both define mathematical objects through finite operations
\item Both are based on the principle of distinguishable information
\end{enumerate}

\textbf{Proof of Equivalence}:

\textbf{Step 1: Traditional mathematics as symbol system}
Let $M_{trad}$ be the set of all mathematical objects expressible in traditional mathematics.
Each $x \in M_{trad}$ must be:
\begin{itemize}
\item Defined by finite symbols (axioms, definitions)
\item Manipulated through finite rules (logical inference)
\item Communicated via finite representations (proofs, calculations)
\end{itemize}

Therefore: $M_{trad} \subseteq \{\text{objects definable by finite symbol sequences}\}$

\textbf{Step 2: Bidirectional Proof of Equivalence}

\textbf{Strict Construction of Bijection}:
Let $\mathcal{S}_{fin}$ be the set of finite symbol sequences, and $\mathbb{N}$ be the set of natural numbers.

\textbf{Lemma 1.3}: There exists a bijection $G: \mathcal{S}_{fin} \to \mathbb{N}$ (Godel encoding)
\label{lem:1.3}
\textbf{Lemma 1.4}: There exists a bijection $\phi: \mathbb{N} \to \Phi$, where $\Phi$ is the set of $\phi$-representations (Zeckendorf theorem)
\label{lem:1.4}

\textbf{Composite Bijection}: $\phi \circ G: \mathcal{S}_{fin} \to \Phi$

For any $x \in M_{trad}$:
\begin{itemize}
\item $x$ is defined by finite symbol sequence $s \in \mathcal{S}_{fin}$ (finiteness of mathematical definitions)
\item $G(s) = n \in \mathbb{N}$ (Godel encoding)
\item $\phi(n) \in \Phi$ ($\phi$-representation)
\end{itemize}

Therefore: $M_{trad} \to \Phi$ (injective mapping)

Similarly, any $\phi$-representation corresponds to:
\begin{itemize}
\item A natural number (by Zeckendorf)
\item A symbol sequence (by inverse Godel encoding)
\item A potential mathematical definition
\end{itemize}

Therefore: $\Phi \to M_{trad}$ (the range covers all definable objects)

\textbf{Step 3: Conclusion}
From bidirectional inclusion, $M_{trad} = M_\phi$. $\square$

\textbf{Philosophical Position: The Essence of Information}

\textbf{Definition 1.2 (The Essence of Information)}:
\label{def:1.2}
In our theoretical framework, information has a trinity essence:
\begin{equation}
\text{Information} \equiv \text{Distinguishability} \equiv \text{Representability}
\end{equation}

\textbf{Rigorous Proof of This Equivalence}:

\textbf{(Distinguishability $\Rightarrow$ Representability)}:
If $x$ is distinguishable from $y$, there must exist some description $d_x \neq d_y$.
These descriptions are representations, hence distinguishability implies representability.

\textbf{(Representability $\Rightarrow$ Distinguishability)}:
If $x$ can be represented as $r_x$, and this representation differs from all others,
then $x$ is distinguishable through its unique representation.

\textbf{(Information $\Leftrightarrow$ Distinguishability)}:
This is our definition: information exists precisely when distinction exists.
No distinction = no information; complete distinction = complete information.

Therefore, these three concepts form a trinity, each implying the others. $\square$

\textbf{Implications}:
\begin{enumerate}
\item There is no "unrepresentable" information in our framework
\item All meaningful distinctions can be encoded
\item The $\phi$-representation system is complete for information encoding
\end{enumerate}

\section{Self-Reference and Paradox Resolution}
\label{sec:ch01_axiom_and_derivation:self-reference-and-paradox-resolution}

Traditional paradoxes (Russell's paradox, liar paradox, etc.) arise from unrestricted self-reference. Our framework resolves these through temporal stratification:

\textbf{Key Insight}: Self-reference unfolds over time, not instantaneously.

When the system describes itself at time $t$, it creates description $d_t$.
This description becomes part of the system at time $t+1$: $d_t \in S_{t+1}$.
The system can then describe $d_t$ at time $t+1$, creating $\text{Desc}(d_t)$.

Paradoxes are avoided because:
\begin{enumerate}
\item No instantaneous self-containment (would violate Lemma 1.1)
\item Descriptions are temporally indexed
\item Each recursive level occupies a distinct time step
\end{enumerate}

\textbf{Example - Resolving the Liar Paradox}:
Statement: "This statement is false"

In our framework:
- Time $t$: System contains statement $s$
- Time $t+1$: System produces description $\text{Desc}(s)$ evaluating truth value
- Time $t+2$: System can describe the description: $\text{Desc}(\text{Desc}(s))$

No paradox arises because truth evaluation and self-reference occur at different times.

\section{Thermodynamic Connection}
\label{sec:ch01_axiom_and_derivation:thermodynamic-connection}

Our entropy definition connects to thermodynamic entropy through information theory:

\textbf{Shannon Entropy}: $H = -\sum p_i \log p_i$

In our framework, if all descriptions are equally likely:
$p_i = 1/|D_t|$, yielding $H = \log |D_t|$

This matches our definition exactly, showing our entropy is the Shannon entropy under uniform distribution.

\textbf{Physical Interpretation}:
\begin{itemize}
\item Growing $|D_t|$ = increasing microscopic configurations
\item Entropy increase = second law of thermodynamics
\item Self-referential systems naturally exhibit thermodynamic behavior
\end{itemize}

\section{Significance of This Derivation}
\label{sec:ch01_axiom_and_derivation:significance-of-this-derivation}

We have proven that the unique axiom is internally consistent and leads to rich consequences:

\begin{itemize}
\item Time emerges as logical necessity, not physical assumption
\item Information, observers, and asymmetry are unified concepts
\item Paradoxes resolve through temporal stratification
\item The arrow of time becomes a logical necessity rather than an empirical fact
\end{itemize}

\section{Emergence of the Concept of Information}
\label{sec:ch01_axiom_and_derivation:emergence-of-the-concept-of-information}

In our theoretical framework, "information" is not a presupposed concept but necessarily emerges from the unique axiom.

\textbf{Theorem 1.4 (Emergence of Information)}
\label{thm:1.4}
Self-referentially complete systems necessarily produce the concept of information.

\textbf{Proof}:
Let system S satisfy self-referential completeness.

\begin{enumerate}
\item \textbf{Necessity of Distinction}:
\end{enumerate}
  From the completeness requirement: $\forall s_1, s_2 \in S: s_1 \neq s_2 \Rightarrow \text{Desc}(s_1) \neq \text{Desc}(s_2)$
  
  This means different states must have different descriptions, i.e., be distinguishable.

\begin{enumerate}
\item \textbf{Emergence of the Concept of Information}:
\end{enumerate}
  Define information as the distinguishability within the system:
  
\begin{equation}
\text{Info}(x) \equiv \exists y \in S: x \neq y \land \text{Desc}(x) \neq \text{Desc}(y)
\end{equation}
  
  That is: information is elements in the system that can be distinguished by the description function.
\begin{enumerate}
\item \textbf{Equivalent Characterization of Information}:
\end{enumerate}
  
  \textbf{Lemma 1.5}: The following three conditions are equivalent:
\label{lem:1.5}
  - (a) x carries information: $\text{Info}(x)$
  - (b) x can be axiomatized: $\exists \mathcal{A}: \mathcal{A} \vdash x$
  - (c) x can be encoded: $\exists e: S \to \mathbb{N}, e(x) \neq e(y)$ when $x \neq y$
  
  \textbf{Proof}:
  - (a)$\Rightarrow$(b): Distinguishable structures can be characterized by distinguishing rules (axioms)
  - (b)$\Rightarrow$(c): Axiom systems can be Godel encoded as natural numbers
  - (c)$\Rightarrow$(a): Different encodings imply distinguishability

Therefore, information necessarily emerges in self-referentially complete systems. $\square$

\textbf{Deep Significance}:
\begin{enumerate}
\item Information is not introduced externally but emerges from self-referential structure
\item Information = Distinguishability = Encodability (trinity)
\item This provides an ontological foundation for information theory
\end{enumerate}

\section{Chapter Outline}
\label{sec:ch01_axiom_and_derivation:chapter-outline}

To prove the absolute completeness of $\phi$-representation:

\begin{itemize}
\item \textbf{Section 1}: Introduction and axiom formulation
\item \textbf{Section 2}: From self-referential completeness to entropy increase
\item \textbf{Section 3}: From entropy increase to information encoding requirements
\item \textbf{Section 4}: From optimal encoding to $\phi$-representation
\item \textbf{Section 5}: Conclusions and prospects
\end{itemize}

\section{Deep Proof of Equivalence}
\label{sec:ch01_axiom_and_derivation:deep-proof-of-equivalence}

Let us rigorously prove the equivalence of entropy increase, asymmetry, time, information, and observers.

\textbf{Theorem 1.5 (Five-fold Equivalence)}
\label{thm:1.5}
For a self-referentially complete system S, the following five propositions are equivalent:
\begin{enumerate}
\item Entropy increase: $\forall t: H(S_{t+1}) > H(S_t)$
\item State asymmetry: $\forall t: S_{t+1} \neq S_t$  
\item Time existence: $\exists \tau: \mathcal{S} \times \mathcal{S} \to \mathbb{R}^+$ (time metric)
\item Information emergence: $\exists I: \mathcal{S} \to \mathcal{I}$ (information mapping)
\item Observer existence: $\exists O \subseteq S: O \times S \to \mathcal{M}$ (measurement mapping)
\end{enumerate}

\textbf{Deep Proof}: We establish deep connections between these concepts.

\textbf{(1)$\Rightarrow$(2) Entropy forces asymmetry}:
Entropy is defined as $H(S_t) = \log |D_t|$ where $D_t$ is the set of descriptions.
If $S_{t+1} = S_t$, then the describable content remains unchanged: $D_{t+1} = D_t$.
Thus $H(S_{t+1}) = H(S_t)$, contradicting the premise.
Therefore $S_{t+1} \neq S_t$.

\textbf{(2)$\Rightarrow$(3) Asymmetry defines time}:
Given state asymmetry, we can define a time metric:
\begin{equation}
\tau(S_i, S_j) = \inf\{n \in \mathbb{N}: \Phi^n(S_i) \cap \Phi^m(S_j) \neq \emptyset \text{ for some } m\}
\end{equation}

This measures the minimum steps needed for two states to share common evolutionary descendants.
$\tau$ satisfies:
- Non-negativity: $\tau(S_i, S_j) \geq 0$
- Identity: $\tau(S_i, S_i) = 0$  
- Symmetry: $\tau(S_i, S_j) = \tau(S_j, S_i)$
- Triangle inequality: $\tau(S_i, S_k) \leq \tau(S_i, S_j) + \tau(S_j, S_k)$

\textbf{(3)$\Rightarrow$(4) Time carries information}:
Given time metric $\tau$, we can define information as what varies with time:
\begin{equation}
I(s) = \{(d, t): d = \text{Desc}_t(s), t \in \mathbb{N}\}
\end{equation}

Each state $s$ has an information trajectory through time.
Different states have different trajectories (by injectivity of Desc).
Thus information emerges as the time-indexed description structure.

\textbf{(4)$\Rightarrow$(5) Information requires observers}:
If information mapping $I$ exists, it must be realized by some mechanism.
This mechanism must:
- Distinguish different information states
- Process/record these distinctions
- Be part of the system (by self-referential completeness)

Define observers as subsystems implementing this mechanism:
\begin{equation}
O = \{o \in S: \exists f_o: I(\mathcal{S}) \to \mathcal{R}_o\}
\end{equation}
where $f_o$ is the observation function and $\mathcal{R}_o$ is the response space.

\textbf{(5)$\Rightarrow$(1) Observers generate entropy}:
Each observation creates a record within the system.
At time $t$, observer $o$ observes state $s$, creating record $r_{o,s,t}$.
This record must be distinguishable from others (different observations yield different records).

The set of records grows over time:
\begin{equation}
R_{t+1} = R_t \cup \{r_{o,s,t}: o \in O, s \in S_t\}
\end{equation}

New records require new descriptions, expanding $D_t$:
\begin{equation}
D_{t+1} \supseteq D_t \cup \{\text{Desc}(r): r \in R_{t+1} \setminus R_t\}
\end{equation}

Therefore:
\begin{equation}
H(S_{t+1}) = \log |D_{t+1}| > \log |D_t| = H(S_t)
\end{equation}

\textbf{Philosophical Insight}: These five concepts are not independent but different facets of a single phenomenon - the self-referential unfolding of existence.

\textbf{(3)$\rightarrow$(4) Time as Information Accumulation}:
The existence of time $\tau$ creates temporal ordering.
This ordering enables information accumulation:
\begin{equation}
I(S_{t+1}) = I(S_t) \cup \{(\text{Desc}(S_t \to S_{t+1}), \tau(S_t, S_{t+1}))\}
\end{equation}

\textbf{(4)$\Rightarrow$(5) Information recognition requires observers}:
The existence of information presupposes recognition and processing mechanisms.

\textbf{Lemma 1.6}: If there exists an information mapping $I: \mathcal{S} \to \mathcal{I}$, then there must exist a subsystem that processes this information.
\label{lem:1.6}

\textbf{Proof}: Information $I(S)$ must be recognized by "something" to be meaningful. This "something" must:
\begin{itemize}
\item Be able to distinguish different information: $\exists \text{dist}: \mathcal{I} \times \mathcal{I} \to \mathbb{R}$
\item Be able to process information: $\exists \text{proc}: \mathcal{I} \to \mathcal{R}$ (some response)
\item Be part of the system: Otherwise violates self-referential completeness
\end{itemize}

Define observers as subsystems with these capabilities:
\begin{equation}
O = \{o \in S: \exists (f_{\text{dist}}, f_{\text{proc}}): o \text{ can recognize and process } I(S)\}
\end{equation}

\textbf{(5)$\Rightarrow$(1) Observation necessarily increases entropy}:
This is the most profound link. The existence of observers necessarily leads to entropy increase.

\textbf{Theorem 1.6 (Observation Increases Entropy)}:
\label{thm:1.6}
If $O \subseteq S$ is an observer, then any observation act increases system entropy.

\textbf{Proof}:
Let observer $O$ observe system state $s \in S_t$ at time $t$.

\begin{enumerate}
\item \textbf{Before observation}: System contains state set $S_t$
\end{enumerate}
\begin{enumerate}
\item \textbf{Observation act}: $O$ measures $s$, producing result $m = \mathcal{M}_O(s)$
\end{enumerate}
\begin{enumerate}
\item \textbf{After observation}: System contains $S_t \cup \{m\}$ (measurement result becomes part of system)
\end{enumerate}

Since measurement results are distinguishable for different states:
\begin{equation}
s_1 \neq s_2 \Rightarrow \mathcal{M}_O(s_1) \neq \mathcal{M}_O(s_2) \text{ (with high probability)}
\end{equation}

New distinguishable elements require new descriptions:
\begin{equation}
D_{t+1} = D_t \cup \{\text{Desc}(m): m \in \text{new measurements}\}
\end{equation}

Therefore:
\begin{equation}
H(S_{t+1}) = \log |D_{t+1}| > \log |D_t| = H(S_t)
\end{equation}

\textbf{Key Insight}: Observation is not passive but active. The very act of observation adds information to the system. This is inevitable because:
\begin{itemize}
\item Observation results must be recorded
\item The record itself increases the system's state space
\item This is the essence of self-referential structure: self-observation necessarily self-expands
\end{itemize}

Therefore the five propositions form an equivalence class. $\square$

\textbf{Corollary 1.1}:
\label{cor:1.1}
In self-referentially complete systems, choosing any proposition as the foundation allows derivation of the other four. This reveals the deep unity of these concepts.